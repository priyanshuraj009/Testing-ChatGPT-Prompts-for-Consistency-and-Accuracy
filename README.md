# ğŸ§  Testing ChatGPT Prompts for Consistency and Accuracy

This project explores how different prompt styles (Direct, Instructional, and Contextual) affect the accuracy, creativity, and consistency of ChatGPTâ€™s responses.

---

## ğŸ¯ Objective
To evaluate and compare how prompt design influences output quality and response reliability in generative AI models.

---

## âš™ï¸ Methodology
1. Selected a simple use case â€” generating product descriptions.
2. Tested three prompt styles:
   - **A. Direct:** "Write a product description for wireless earbuds."
   - **B. Instructional:** "You are a professional copywriter. Write a 3-sentence product description focusing on sound and battery life."
   - **C. Contextual:** "Imagine youâ€™re writing for Amazon. Create an engaging and informative product description focusing on comfort, battery life, and sound."
3. Rated responses manually (1â€“5 scale) for:
   - Accuracy
   - Creativity
   - Consistency

---

## ğŸ“Š Results

| Prompt Type | Accuracy | Creativity | Consistency | Avg Score |
|--------------|-----------|-------------|--------------|------------|
| A â€“ Direct | 3.5 | 3.0 | 4.0 | 3.5 |
| B â€“ Instructional | 4.0 | 4.2 | 4.5 | 4.2 |
| C â€“ Contextual | 4.5 | 4.8 | 4.6 | 4.6 |

---

## ğŸ§© Key Insights
- Contextual prompts improved output quality by ~30%.
- Role-based and structured prompts lead to more reliable answers.

---

## ğŸ§° Tools & Skills
- ChatGPT (GPT-4)
- Manual Evaluation (QA metrics)
- Data Interpretation, Prompt Engineering, Analytical Writing

---

## ğŸ“ˆ Conclusion
Context and structure are crucial in prompt engineering. Giving AI a clear role and objective yields more accurate, coherent results.
